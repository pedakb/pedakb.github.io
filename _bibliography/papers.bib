---
---

@string{aps = {American Physical Society,}}

@article{yan2025sigmoid,
  title={Sigmoid Self-Attention is Better than Softmax Self-Attention: A Mixture-of-Experts Perspective},
  author={Yan, Fanqi and Nguyen, Huy and Akbarian, Pedram and Ho, Nhat and Rinaldo, Alessandro},
  journal={arXiv:2502.00281},
  year={2025},
  abbr={Preprint},
  note={Under review},
  selected={true},
  ArXiv={2502.00281}
}

@article{akbarian2024quadratic,
    title={Quadratic Gating Functions in Mixture of Experts: A Statistical Insight},
    author={Akbarian*, Pedram and Nguyen*, Huy and Han*, Xing and Ho, Nhat},
    journal={arXiv:2410.11222},
    year={2024},
    abbr={Preprint},
    keywords={preprint},
    note={Under review},
    selected={true},
    ArXiv={2410.11222}
}

@inproceedings{
  yan2025understanding,
  title={Understanding Expert Structures on Minimax Parameter Estimation in Contaminated Mixture of Experts},
  author={Fanqi Yan and Huy Nguyen and Le Quang Dung and Pedram Akbarian and Nhat Ho},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2025},
  abbr={AISTATS},
  ArXiv={2410.12258}
}

@inproceedings{nguyen2024statistical,
    title={Statistical Advantages of Perturbing Cosine Router in Sparse Mixture of Experts},
    author={Nguyen, Huy and Akbarian*, Pedram and Pham*, Trang and Nguyen, Trang and Zhang, Shujian and Ho, Nhat},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2025},
    abbr={ICLR},
    ArXiv={2405.14131},
    selected={true}
}


@inproceedings{akbarianimproving,
  title={Improving Computational Complexity in Statistical Models with Local Curvature Information},
  author={Akbarian*, Pedram and Ren*, Tongzheng and Zhuo, Jiacheng and Ho, Nhat and others},
  year={2024},
  booktitle={International Conference on Machine Learning (ICML)},
  abbr={ICML},
  pdf={https://openreview.net/pdf?id=KwgAThfxEd},
  selected={true}
}

@inproceedings{nguyen2024temperature,
  title={Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?}, 
  author={Huy Nguyen and Pedram Akbarian and Nhat Ho},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2024},
  ArXiv={2401.13875},
  abbr={ICML},
  selected={true}
}

@inproceedings{
anonymous2024a,
title={A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts},
author={Huy Nguyen and Pedram Akbarian and TrungTin Nguyen and Nhat Ho},
booktitle={International Conference on Machine Learning (ICML)},
year={2024},
url={https://openreview.net/forum?id=2Sl0lPF6ka},
ArXiv={2310.14188},
abbr={ICML},
selected={true}
}

@inproceedings{
  nguyen2024statistical,
  title={Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts},
  author={Huy Nguyen and Pedram Akbarian and Fanqi Yan and Nhat Ho},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://openreview.net/forum?id=jvtmdK69KQ},
  ArXiv={2309.13850},
  abbr={ICLR},
  selected={true}
}

@inproceedings{
  anonymous2022improving,
  title={Improving Counterfactual Explanations for Time Series Classification Models in Healthcare Settings},
  author={Tina Han and Jette Henderson and Pedram Akbarian and Joydeep Ghosh},
  booktitle={NeurIPS 2022 Workshop on Learning from Time Series for Health},
  year={2022},
  url={https://openreview.net/forum?id=eEk220kuAlz},
  pdf={https://openreview.net/forum?id=eEk220kuAlz},
  abbr={NeurIPS}
}
